# -*- coding: utf-8 -*-
"""nlp_projectSPAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SoFbOwLN61RIat-eUtNKPGcA4bEaVMln

**IMPORTING LIBRARIES**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import nltk

import re
from collections import Counter
from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import FreqDist
from nltk import pos_tag

"""**IMPORTING BOOK**"""

from google.colab import files
file_path='/content/count of monte cristo.txt'
with open(file_path, 'r') as file:
 TextT = file.read()

"""**PRINT BOOK**"""

print("File Content:")
print(TextT)

"""**NUMBER OF WORDS IN BOOK**"""

len(TextT)

"""**DATATYPE OF BOOK**"""

type(TextT)

"""**DATA PREPROCESSING**

**REMOVING CHAPTER NUMBER AND NAME**
"""

pattern = r'Chapter \d+: [^\n]*'

clean_text1 = re.sub(pattern, '', TextT)

print(clean_text1)

"""**REMOVING PAGE NUMBERS AND PUNCTUATION MARKS**"""

import re

# Define regular expression patterns
afterword_pattern = re.compile(r'AFTERWORD.*', re.DOTALL)
punctuation_pattern = re.compile(r'[^\w\s]')
page_number_pattern = re.compile(r'\b(?:[1-9]|[1-9]\d|1\d{2}|200|2[0-9][0-9]|300)\b')

# Remove page numbers from clean_text1
text_without_page_numbers = re.sub(page_number_pattern, '', clean_text1)

# Remove content after "AFTERWORD" and everything that follows
text_without_afterword = re.sub(afterword_pattern, '', text_without_page_numbers)

# Remove punctuation from the text
cleaned_text = re.sub(punctuation_pattern, '', text_without_afterword)

# Print the pre-processed text
print("Pre-processed Text:")
print(cleaned_text)

"""**TOKENIZATION**"""

from nltk.tokenize import word_tokenize

import nltk
nltk.download('punkt')

words = word_tokenize(cleaned_text)

# Create a WordCloud object with specified parameters
wordcloud = WordCloud(
    width=3000,           # Width of the word cloud image
    height=2000,          # Height of the word cloud image
    background_color='white',  # Background color of the word cloud image
    colormap='plasma',    # Color map for the word cloud
    stopwords={},         # Set of words to exclude from the word cloud
).generate(cleaned_text)

# Create a figure for displaying the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")  # Turn off the axis labels
plt.title("Word Cloud on Tokens including Stopwords")
plt.show()

"""**REMOVING STOPWORDS**"""

from nltk.corpus import stopwords

import nltk
nltk.download('stopwords')

stop_words=set(stopwords.words('english'))

nltk.download('punkt')

# Tokenize the text and remove common stop words
tokenized_words = [word for word in words if word.lower() not in stop_words]

# Display the tokenized text without stop words
print("Tokenized Text after Stop Words Removal:")
print(tokenized_words)

tokenized_words[:10]       #showing 10 tokenized words

len(tokenized_words)    #total number of tokenized words

len(stop_words)          #total number of stopwords

"""**FREQUENCY DISTRIBUTION ANALYSIS OF TOKENS**"""

# Create a frequency distribution of tokenized words
frequency = FreqDist(tokenized_words)

# Get the 10 most common words and their frequencies
most_common = frequency.most_common(10)

# Display the most common words and their frequencies
print("Top 10 Most Common Words and Their Frequencies:")
for word, freq in most_common:
    print(f"{word}: {freq}")

# Create a plot of token frequency distribution
plt.title("Token Frequency Distribution")
frequency.plot(30, cumulative=False)

"""**WORD CLOUD ON TOKENS**"""

# Create a word cloud based on tokenized words
wordcloud = WordCloud(
    width=3000,                # Width of the word cloud image
    height=2000,               # Height of the word cloud image
    background_color='white',  # Background color of the word cloud
    colormap='autumn',         # Color map for the word cloud
    stopwords={},              # Set of words to exclude from the word cloud
).generate(" ".join(tokenized_words))  # Combine tokenized words into a single string

# Create a figure for displaying the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")  # Turn off the axis labels
plt.title("Word Cloud on Tokens")
plt.show()

"""**POS TAGGING USING PENN TREEBANK TAG SET**"""

# Download the NLTK data for part-of-speech tagging
import nltk
nltk.download('averaged_perceptron_tagger')

# Combine tokenized words into a single string
book = " ".join(tokenized_words)

# Tokenize the combined string into words
book_words = word_tokenize(book)

# Perform part-of-speech tagging on the words in the book
book_tagged_pos_tags = nltk.pos_tag(book_words)

# Access the entire list of part-of-speech tagged words in the 'book_tagged_pos_tags' variable
book_tagged_pos_tags[:]

"""**POS TAGS AND THEIR FREQUENCIES SHOWN USING GRAPH**"""

import nltk

def tag_distribution(text):
    # Calculate the frequency distribution of tags
    tag_freq = nltk.FreqDist(tag for word, tag in text)

    return tag_freq

book_tag_freq_penn = tag_distribution(book_tagged_pos_tags)

# Set the figure size to 10x5 inches
plt.figure(figsize=(10, 5))

# Plot the top 20 tags from the Penn Treebank tag distribution
book_tag_freq_penn.plot(20, title='Penn Treebank Tag Distribution', linestyle='-.', color='blue')

# Display the plot
plt.show()

"""**POS TAGS AND THEIR FREQUENCIES USING BAR PLOT**"""

# Function to perform PoS tagging using the Penn Treebank tag set
def tag_distribution(text):
    tagged_text = nltk.pos_tag(text)
    tag_freq = nltk.FreqDist(tag for (word, tag) in tagged_text)
    return tag_freq

# Calling PoS tsagging function
tag_freq = tag_distribution(tokenized_words)
print(tag_freq)

# extracting the PoS (Parts of Speech) tags and their frequencies from frequency distribution
PoS_Tags, frequencies = zip(*tag_freq.most_common())


# Creating the bar plot
plt.figure(figsize=(10, 5))
plt.bar(PoS_Tags, frequencies)

plt.xlabel("POS Tags")
plt.ylabel("Frequency")
plt.title("Frequency Distribution of POS Tags")

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Function to split text into chapters
def split_into_chapters(text):
    chapters = re.split(r'Chapter \d+', text)
    # Remove empty chapters
    chapters = [chapter.strip() for chapter in chapters if chapter.strip()]
    return chapters

# Function to create a bigram probability table
def create_bigram_probability_table(tokens):
    bigrams_list = list(bigrams(tokens))
    bigram_freq = nltk.FreqDist(bigrams_list)
    bigram_prob = []

    for bigram, freq in bigram_freq.items():
        word1, word2 = bigram
        probability = freq / len(bigrams_list)
        bigram_prob.append((word1, word2, probability))

    return bigram_prob

# Assuming you have a `preprocess_text` function defined for text preprocessing
def preprocess_text(text):
    # Perform text preprocessing here
    # This is where you might remove stopwords, punctuation, and perform other cleaning steps
    return text

# Assuming you have a 'novel_text' variable containing the text of the novel
novel_text = TextT

# Split the novel into chapters
chapters = split_into_chapters(novel_text)

# Find the largest chapter
largest_chapter = max(chapters, key=len)

# Preprocess the largest chapter
largest_chapter = preprocess_text(largest_chapter)

# Tokenize the largest chapter without removing stopwords
tokens1 = word_tokenize(largest_chapter)

# Create a bigram probability table for the largest chapter
bigram_probability = create_bigram_probability_table(tokens1)

# Create a DataFrame to display the bigram probability table
df = pd.DataFrame(data=bigram_probability, columns=['Prev_word', 'Next_word', 'Prob'])
df = df.sort_values(by=['Prob'], ascending=False)

# Display the DataFrame
print(df)

import random
# Function to ask fill-in-the-blank questions
def shannon_game(bigram_probability, original_sentence):
    # Choosing a random bigram from the bigram probability table.
    bigram = random.choices(bigram_probability, weights=[prob for w1, w2, prob in bigram_probability])[0]
    prev_word, next_word, prob = bigram

    # Replacing the next word with a blank in the original sentence.
    question = original_sentence.replace(next_word, "______")

    # The answer is the next word in the bigram.
    answer = next_word

    return question, answer



# Finding chapters other than Chapter C
chapters = split_into_chapters(novel_text)
valid_chapters = [chapter for chapter in chapters if not re.match(r'Chapter C', chapter)]

if valid_chapters:
    # Choosing a random chapter
    random_chapter = random.choice(valid_chapters)
# Tokenizing the chapter without removing stop words
    chapter_tokens = word_tokenize(random_chapter)

    # Asking a fill-in-the-blank question using the bigram probability table and chapter text
    question, answer = shannon_game(bigram_probability, random_chapter)

    # Printing the question to the console.
    print("Fill in the blank:")
    print(question)

    # Getting the user's input.
    user_input = input("Your answer: ")

    # Checking if the user's input is correct.
    if user_input.lower() == answer.lower():
        print("Correct!")
    else:
        print(f"Incorrect. The correct answer is: {answer}")
else:
    print("No valid chapters found (excluding Chapter C).")