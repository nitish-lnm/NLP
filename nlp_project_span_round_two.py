# -*- coding: utf-8 -*-
"""nlp_projectSPANround2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mblqx6xvRjX1u0vIgIW5w-EHN1hTEtbt

**IMPORTING LIBRARIES**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import nltk
import string

"""**IMPORTING BOOK**"""

# Open the text file and create a list called 'Text'
Text=[line.rstrip() for line in open ("count of monte cristo 1.txt")]
# The 'Text' list now contains lines from the file without trailing whitespace

Text[:]

"""**NUMBER OF WORDS IN THE TEXT**"""

len(Text)

"""**DATA TYPE OF THE TEXT**"""

type(Text)

"""**REMOVING THE CHAPTER NO AND NAME FROM TEXT**"""

# Initialize an empty list to store cleaned strings and a flag to control line skipping
Text1 = []
skip_next = False

# Iterate through each line in the 'Text' list
for line in Text:
    # Check if the line contains the word "Chapter"; if so, set a flag to skip the next line
    if "Chapter" in line:
        skip_next = True
    # If the line doesn't contain "Chapter" and skipping is not required
    elif not skip_next:
        # Clean the string by removing punctuation and replacing a specific character
        cleaned_string = ''.join([char for char in line if char not in string.punctuation])
        cleaned_string = cleaned_string.replace('\x0c', '')
        # Check if the cleaned string is not empty, doesn't consist only of digits, and doesn't contain "Chapter"
        if cleaned_string.strip() and not (all(word.isdigit() for word in cleaned_string.split()) or "Chapter" in cleaned_string):
            # Append the cleaned string to the 'Text1' list
            Text1.append(cleaned_string)
    # If the line contains "Chapter," reset the flag to stop skipping
    else:
        skip_next = False

"""**UPDATED TEXT**"""

Text1[:]

"""**NUMBER OF WORDS IN UPDATED TEXT**"""

len(Text1)

"""**CONCATENATION OF TEXT INTO A SINGLE STRING**"""

# Concatenate the cleaned and processed strings in the list 'Text1' into a single string
# The strings are joined with spaces and converted to lowercase
Text_string = (" ".join(Text1)).lower()
Text_string[:]

"""**LENGTH OF STRING**"""

len(Text_string)

"""**IMPORTING spaCy LIBRARY TO PERFORM NER ON TEXT**"""

import spacy
from spacy import displacy

# Load spaCy English language model with specific components disabled for efficiency
NER = spacy.load("en_core_web_sm", disable=["tok2vec", "tagger", "parser", "attribute_ruler", "lemmatizer"])

"""**NAMED ENTITY RECOGNITION**"""

#we had to take limit of string upto 10,00,000 because total length of string were 2479433 and error is occuring of exceeded length
nerTaggedText = NER( Text_string[:1000000] )

# Extract and print a list of tuples containing the text and label of named entities identified by spaCy's NER in the analyzed text
print([(entity.text, entity.label_) for entity in nerTaggedText.ents])

"""**FIRST ROUND**

**EXTRACTING RANDOM PASSAGE AND PERFORMING NER**
"""

# Extract the first 50,000 to 1,00,000 characters from the processed text
Passage1 = Text_string[50000:100000]

# Identify named entities in the extracted paragraph using spaCy's NER
taggedpassage1 = NER(Passage1)

# Visualize the identified entities using spaCy's 'displacy' module
# The visualization is displayed in a Jupyter notebook with a focus on entity annotations
displacy.render(taggedpassage1, jupyter=True, style='ent')

"""**EXTRACTING ENTITIES FROM THE PASSAGE , THEIR NAMES AND LABELS**"""

# Print a list of tuples, each containing the text and label of identified named entities in taggedpassage1
print([(entity.text, entity.label_) for entity in taggedpassage1.ents])

# Extract and print only the text of named entities identified in taggedpassage1
entity_texts = [entity.text for entity in taggedpassage1.ents]
print(entity_texts)

# Extract and print a list of entity labels from taggedpassage1
Labels1 = [entity.label_ for entity in taggedpassage1.ents]
print(Labels1)

"""**COMPARISON WITH MANUAL LABELLING**"""

from sklearn import metrics

# Manually assigned labels for named entities
manual_labels1 = ['ORG', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'TIME', 'NORP', 'NORP', 'DATE', 'DATE', 'CARDINAL', 'TIME', 'GPE', 'GPE', 'GPE', 'ORDINAL', 'DATE', 'QUANTITY', 'TIME', 'ORDINAL', 'FAC', 'CARDINAL', 'GPE', 'CARDINAL', 'ORG', 'ORDINAL', 'NORP', 'CARDINAL', 'DATE', 'GPE', 'TIME', 'NORP', 'CARDINAL', 'ORG', 'TIME', 'CARDINAL', 'TIME', 'TIME', 'CARDINAL', 'TIME', 'DATE', 'TIME', 'DATE', 'CARDINAL', 'NORP', 'TIME', 'GPE', 'DATE', 'DATE', 'DATE', 'ORDINAL', 'ORDINAL', 'DATE', 'ORDINAL', 'CARDINAL', 'DATE', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'PERSON', 'TIME', 'TIME', 'DATE', 'CARDINAL', 'FAC', 'ORDINAL', 'ORDINAL', 'CARDINAL', 'ORDINAL', 'CARDINAL', 'CARDINAL', 'PRODUCT', 'GPE', 'DATE', 'TIME', 'DATE', 'CARDINAL', 'CARDINAL', 'ORG', 'CARDINAL', 'CARDINAL', 'ORDINAL', 'TIME', 'DATE', 'CARDINAL', 'DATE', 'CARDINAL', 'QUANTITY', 'CARDINAL', 'GPE', 'GPE', 'GPE', 'PERSON', 'GPE', 'DATE', 'CARDINAL', 'ORG', 'PERSON', 'CARDINAL', 'CARDINAL', 'ORDINAL', 'DATE', 'DATE', 'GPE', 'ORG', 'PERSON', 'DATE']

# Predicted labels obtained from spaCy's Named Entity Recognition
Labels1 = [entity.label_ for entity in taggedpassage1.ents]

# Evaluate the model performance by generating a classification report
classification_report_result = metrics.classification_report(manual_labels1, Labels1)

# Display the classification report
print(classification_report_result)

"""**SECOND ROUND**

**EXTRACTING RANDOM PASSAGE AND PERFORMING NER**
"""

# Extract the first 10000 to 30000 characters from the processed text
Passage2 = Text_string[10000:30000]

# Identify named entities in the extracted paragraph using spaCy's NER
taggedpassage2 = NER(Passage2)

# Visualize the identified entities using spaCy's 'displacy' module
# The visualization is displayed in a Jupyter notebook with a focus on entity annotations
displacy.render(taggedpassage2, jupyter=True, style='ent')

"""**EXTRACTING ENTITIES FROM THE PASSAGE , THEIR NAMES AND LABELS**"""

# Print a list of tuples, each containing the text and label of identified named entities in taggedpassage2
print([(entity.text, entity.label_) for entity in taggedpassage2.ents])

# Extract and print only the text of named entities identified in taggedpassage2
entity_texts = [entity.text for entity in taggedpassage2.ents]
print(entity_texts)

# Extract and print a list of entity labels from taggedpassage2
Labels2 = [entity.label_ for entity in taggedpassage2.ents]
print(Labels2)

"""**COMPARISON WITH MANUAL LABELLING**"""

from sklearn import metrics

# Manually assigned labels for named entities
manual_labels2 = ['CARDINAL', 'ORDINAL', 'CARDINAL', 'DATE', 'DATE', 'DATE', 'ORDINAL', 'GPE', 'DATE', 'DATE', 'DATE', 'NORP', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'DATE', 'TIME', 'PERSON', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'TIME', 'CARDINAL', 'TIME', 'GPE', 'GPE', 'GPE', 'ORDINAL', 'CARDINAL', 'ORG', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'ORDINAL', 'NORP', 'CARDINAL', 'CARDINAL', 'MONEY', 'DATE', 'QUANTITY', 'MONEY', 'DATE', 'MONEY', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'DATE', 'ORDINAL', 'DATE', 'CARDINAL', 'CARDINAL', 'GPE', 'CARDINAL', 'ORDINAL', 'PERSON', 'DATE', 'CARDINAL', 'ORG', 'CARDINAL', 'TIME', 'CARDINAL', 'DATE', 'CARDINAL', 'CARDINAL', 'GPE', 'DATE', 'DATE', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'NORP', 'ORDINAL', 'CARDINAL', 'DATE']

# Predicted labels obtained from spaCy's Named Entity Recognition
Labels2 = [entity.label_ for entity in taggedpassage2.ents]

# Evaluate the model performance by generating a classification report
classification_report_result = metrics.classification_report(manual_labels2, Labels2)

# Display the classification report
print(classification_report_result)

"""**THIRD ROUND**

**EXTRACTING RANDOM PASSAGE AND PERFORMING NER**
"""

# Extract the 300000 to 400000 characters from the processed text
Passage3 = Text_string[300000:400000]

# Identify named entities in the extracted paragraph using spaCy's NER
taggedpassage3 = NER(Passage3)

# Visualize the identified entities using spaCy's 'displacy' module
# The visualization is displayed in a Jupyter notebook with a focus on entity annotations
displacy.render(taggedpassage3, jupyter=True, style='ent')

"""**EXTRACTING ENTITIES FROM THE PASSAGE , THEIR NAMES AND LABELS**"""

# Print a list of tuples, each containing the text and label of identified named entities in taggedpassage3
print([(entity.text, entity.label_) for entity in taggedpassage3.ents])

# Extract and print only the text of named entities identified in taggedpassage3
entity_texts = [entity.text for entity in taggedpassage3.ents]
print(entity_texts)

# Extract and print a list of entity labels from taggedpassage3
Labels3 = [entity.label_ for entity in taggedpassage3.ents]
print(Labels3)

"""**COMPARISON WITH MANUAL LABELLING**"""

from sklearn import metrics

# Manually assigned labels for named entities
manual_labels3 = ['ORDINAL', 'CARDINAL', 'CARDINAL', 'ORDINAL', 'NORP', 'NORP', 'ORDINAL', 'ORDINAL', 'ORDINAL', 'CARDINAL', 'GPE', 'TIME', 'TIME', 'DATE', 'DATE', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'DATE', 'DATE', 'CARDINAL', 'CARDINAL', 'ORDINAL', 'NORP', 'CARDINAL', 'CARDINAL', 'NORP', 'LANGUAGE', 'NORP', 'DATE', 'DATE', 'CARDINAL', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'ORDINAL', 'DATE', 'ORDINAL', 'DATE', 'CARDINAL', 'DATE', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'TIME', 'CARDINAL', 'TIME', 'CARDINAL', 'NORP', 'TIME', 'ORDINAL', 'ORDINAL', 'DATE', 'ORDINAL', 'ORDINAL', 'CARDINAL', 'TIME', 'TIME', 'NORP', 'TIME', 'DATE', 'TIME', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'ORDINAL', 'CARDINAL', 'NORP', 'DATE', 'DATE', 'DATE', 'CARDINAL', 'CARDINAL', 'ORDINAL', 'DATE', 'ORDINAL', 'ORDINAL', 'CARDINAL', 'ORDINAL', 'DATE', 'TIME', 'CARDINAL', 'DATE', 'DATE', 'GPE', 'DATE', 'GPE', 'PERSON', 'GPE', 'GPE', 'CARDINAL', 'CARDINAL', 'GPE', 'ORDINAL', 'CARDINAL', 'ORDINAL', 'ORDINAL', 'CARDINAL', 'ORG', 'CARDINAL', 'CARDINAL', 'GPE', 'CARDINAL', 'CARDINAL', 'ORG', 'GPE', 'CARDINAL', 'CARDINAL', 'ORDINAL', 'DATE', 'PERSON', 'DATE', 'CARDINAL', 'NORP', 'GPE', 'CARDINAL', 'ORDINAL', 'GPE', 'ORG', 'ORG', 'ORG', 'GPE', 'CARDINAL', 'DATE', 'CARDINAL', 'DATE', 'CARDINAL', 'DATE', 'DATE', 'GPE', 'CARDINAL', 'CARDINAL', 'DATE', 'ORG', 'DATE', 'CARDINAL', 'ORG', 'PERSON', 'CARDINAL', 'DATE', 'CARDINAL', 'CARDINAL', 'DATE', 'DATE', 'GPE', 'MONEY', 'CARDINAL', 'TIME', 'CARDINAL', 'DATE', 'TIME', 'DATE', 'CARDINAL', 'DATE', 'CARDINAL', 'DATE', 'ORDINAL', 'PERSON', 'ORDINAL', 'ORG', 'CARDINAL', 'DATE', 'ORG', 'CARDINAL', 'ORDINAL', 'CARDINAL', 'ORDINAL', 'DATE', 'CARDINAL', 'NORP', 'CARDINAL', 'ORG', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'DATE', 'DATE', 'DATE', 'CARDINAL', 'CARDINAL', 'DATE', 'CARDINAL', 'TIME', 'LOC', 'DATE', 'CARDINAL', 'DATE', 'ORDINAL', 'ORDINAL', 'ORDINAL', 'DATE', 'CARDINAL', 'ORDINAL', 'TIME', 'TIME', 'ORDINAL', 'DATE', 'CARDINAL', 'ORDINAL', 'ORDINAL', 'TIME', 'DATE', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'NORP', 'CARDINAL', 'TIME', 'TIME', 'TIME', 'NORP', 'TIME', 'CARDINAL', 'TIME', 'DATE', 'NORP', 'CARDINAL', 'ORDINAL', 'CARDINAL', 'DATE', 'DATE', 'ORDINAL', 'NORP', 'CARDINAL', 'DATE', 'DATE', 'TIME', 'CARDINAL', 'DATE', 'DATE', 'TIME', 'CARDINAL', 'ORDINAL', 'DATE', 'TIME', 'TIME', 'TIME', 'ORDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'TIME', 'ORDINAL', 'TIME', 'CARDINAL', 'CARDINAL', 'ORDINAL', 'CARDINAL', 'CARDINAL', 'DATE', 'CARDINAL', 'ORDINAL', 'ORDINAL', 'TIME', 'CARDINAL', 'ORDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'DATE', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'DATE', 'ORDINAL', 'CARDINAL', 'ORDINAL', 'CARDINAL', 'CARDINAL', 'TIME', 'TIME', 'TIME', 'ORDINAL', 'CARDINAL', 'TIME', 'CARDINAL', 'ORDINAL', 'CARDINAL', 'ORDINAL', 'ORDINAL', 'CARDINAL', 'TIME', 'NORP', 'TIME', 'TIME', 'TIME', 'CARDINAL', 'GPE', 'GPE', 'QUANTITY', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'NORP', 'ORDINAL', 'ORDINAL', 'ORDINAL', 'DATE', 'NORP', 'NORP', 'NORP', 'TIME', 'CARDINAL', 'QUANTITY', 'DATE', 'DATE', 'ORDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'PERSON', 'ORDINAL', 'TIME']

# Predicted labels obtained from spaCy's Named Entity Recognition
Labels3 = [entity.label_ for entity in taggedpassage3.ents]

# Evaluate the model performance by generating a classification report
classification_report_result = metrics.classification_report(manual_labels3, Labels3)

# Display the classification report
print(classification_report_result)





from matplotlib import pyplot as plt
from pandas import DataFrame
from matplotlib.cm import ScalarMappable

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# Read the entire text file
with open("count of monte cristo 1.txt", "r") as f:
    chapters_text = f.read()

# Split the text into separate chapters based on your chosen delimiter (e.g., "---", "##")
chapters = chapters_text.split("---")

# Create a TF-IDF vectorizer object
vectorizer = TfidfVectorizer()

# Fit and transform the chapters to TF-IDF vectors
chapter_vectors = vectorizer.fit_transform(chapters)

for chapter_index, chapter_vector in enumerate(chapter_vectors):
    # Access the individual chapter vector using its index
    tf_idf_matrix = chapter_vector.toarray()

    # ... use `tf_idf_matrix` for your analysis
    # ... (e.g., print specific terms and weights)

    print(f"Chapter {chapter_index + 1} TF-IDF Matrix:")
    print(tf_idf_matrix)


# Calculate the cosine similarity matrix
similarity_matrix = cosine_similarity(chapter_vectors)

# Print the matrix for viewing
print("\n Cosine similarity matrix:")
print(similarity_matrix)

# Find the most similar chapters for each one
for chapter_index, _ in enumerate(chapters):
    # Find the index of the most similar chapter (excluding itself)
    most_similar_index = np.argmax(similarity_matrix[chapter_index, :])
    if most_similar_index != chapter_index:
        most_similar_chapter = chapter_index+1  # adjust based on how chapter indices are counted

        # Print the most similar chapter
        print(f"Chapter '{chapter_index+1}' is most similar to '{most_similar_chapter}' (similarity: {similarity_matrix[chapter_index, most_similar_index]})")

df = DataFrame(similarity_matrix, columns=[f"Chapter {i+1}" for i in range(len(chapters))], index=[f"Chapter {i+1}" for i in range(len(chapters))])
cmap = plt.cm.cool # adjust the colormap as needed (e.g., Blues, Orangered)
mapper = ScalarMappable(cmap=cmap, norm=plt.Normalize(0, 1))
mapper.set_array([])
fig, ax = plt.subplots(figsize=(10, 10))
table = ax.table(colLabels=df.columns, cellText=df.values, cellLoc="center", cellColours=mapper.to_rgba(df.values))
table.auto_set_font_size(False)
table.set_fontsize(20)
table.scale(10, 10)
ax.set_title("Chapter Similarity Gradient Table")
#ax.colorbar(mapper, label="Cosine Similarity", orientation="vertical")
plt.show()